{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"WeatherVista Project - Session 5: Web Scraping and Using Additional APIs\"\n",
        "execute: \n",
        "  enabled: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "- Learn the basics of web scraping using BeautifulSoup.\n",
        "- Understand ethical considerations and legal aspects of web scraping.\n",
        "- Explore and use additional APIs to gather more weather-related data.\n",
        "\n",
        "## 1. Introduction to Web Scraping\n",
        "Web scraping involves extracting data from websites. It is important to adhere to ethical guidelines and terms of service of websites.\n",
        "\n",
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Web Scraping with BeautifulSoup\n",
        "We'll scrape weather-related data from a sample website.\n",
        "\n",
        "### Fetching the HTML Content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://www.example.com/weather'\n",
        "response = requests.get(url)\n",
        "html_content = response.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing the HTML Content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "print(soup.prettify())  # Print the formatted HTML content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting Specific Data\n",
        "Assuming the website has a table with weather data, we'll extract the table content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "table = soup.find('table', {'class': 'weather_table'})\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "weather_data = []\n",
        "for row in rows[1:]:  # Skip the header row\n",
        "    cols = row.find_all('td')\n",
        "    data = {\n",
        "        'Date': cols[0].text,\n",
        "        'Temperature': cols[1].text,\n",
        "        'Humidity': cols[2].text,\n",
        "        'Condition': cols[3].text\n",
        "    }\n",
        "    weather_data.append(data)\n",
        "\n",
        "weather_df = pd.DataFrame(weather_data)\n",
        "weather_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using Additional APIs\n",
        "We'll explore additional APIs to fetch more weather-related data, such as air quality or UV index.\n",
        "\n",
        "### Air Quality API Example\n",
        "We'll use the AirNow API to fetch air quality data.\n",
        "\n",
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetching Air Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "api_key = 'your_airnow_api_key'\n",
        "url = f\"http://www.airnowapi.org/aq/observation/zipCode/current?format=application/json&zipCode=90210&distance=25&API_KEY={api_key}\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    air_quality_data = response.json()\n",
        "    air_quality_data\n",
        "else:\n",
        "    print(f\"Failed to fetch data: {response.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing Air Quality Data\n",
        "We'll parse the JSON response and create a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "air_quality_df = pd.DataFrame(air_quality_data)\n",
        "air_quality_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Combining Web Scraping and API Data\n",
        "We'll combine the scraped weather data and the data fetched from additional APIs.\n",
        "\n",
        "### Merging DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "combined_df = pd.merge(weather_df, air_quality_df, left_on='Date', right_on='Date', how='inner')\n",
        "combined_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Homework\n",
        "- Explore other weather-related websites and practice web scraping to gather data.\n",
        "- Experiment with additional APIs to fetch various types of weather data.\n",
        "\n",
        "## Summary\n",
        "In this session, we learned the basics of web scraping using BeautifulSoup and explored additional APIs to fetch more weather-related data. We also combined the data from web scraping and APIs to create a comprehensive dataset.\n",
        "\n",
        "Next session, we will focus on creating a GUI for the WeatherVista project using Tkinter or Jupyter Widgets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}